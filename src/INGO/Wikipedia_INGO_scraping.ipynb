{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia list of INGO web scraping\n",
    "\n",
    "Some charities can be considered as \"International Non-Governmental Organization\" and may not appear directly in the wikipedia \"charitable foundation\" list. For further information about our datascraping strategy, please refer to \"../charities/Wikipeida_harities_scraping.ipynb'. This allows us to complete our dataset with further more possible charities. We scrape this data off the wikipedia page using the beautiful soup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the wikipedia list of INGO in html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get html webpage\n",
    "URL = 'https://en.wikipedia.org/wiki/International_non-governmental_organization'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract name of INGO and link to its wikipedia page from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find page section containing links\n",
    "link_sections = soup.findAll('div', class_=\"mw-parser-output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-700bde7180a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extracts only wikipedia links\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlinks_lists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink_sections\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mflat_links_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks_lists\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-700bde7180a0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extracts only wikipedia links\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlinks_lists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink_sections\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mflat_links_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks_lists\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Extracts only wikipedia links\n",
    "links_lists = [x.findAll('a', attrs={'href': re.compile(\"/w\")}) for x in link_sections]\n",
    "\n",
    "flat_links_list = [((item.text),(item.get('href'))) for sublist in links_lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and flattening of the list\n",
    "'''\n",
    "As the wikipedia html page is not well formated, the relevant informations are extracted\n",
    "with visual aid.\n",
    "'''\n",
    "flat_links_list = flat_links_list[44:97]\n",
    "\n",
    "for tag in flat_links_list[:]:\n",
    "    if 'edit' in tag:\n",
    "        flat_links_list.remove(tag)\n",
    "        \n",
    "flat_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a dataframe containing INGO name and wikipedia link\n",
    "INGO = pd.DataFrame([(x[0], 'https://en.wikipedia.org' + x[1]) for x in flat_links_list], columns=('Name', 'Link'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scraped link list for further processing\n",
    "INGO.to_csv('../../generated/INGO/wikipedia_INGO_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract information about individual INGO from the table on their linked pages\n",
    "\n",
    "Now we want to extract information about individual INGOs from the navbox on their linked pages. We use the navbox because it is already semi-structured, making scraping easier. But we need to know what we are looking for, so we collect the available features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_features(INGO):\n",
    "    '''\n",
    "    Goes through a list of wikipedia pages and extracts the features shown in the table\n",
    "    \n",
    "    input: dataframe including a 'Link' column of wikipedia links as strings\n",
    "    output: a Series of strings\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for index, row in INGO.iterrows():\n",
    "        link = row['Link']\n",
    "        r_link = requests.get(link)\n",
    "        soup_link = BeautifulSoup(r_link.text, 'html.parser')\n",
    "        table = soup_link.find('table', class_=\"infobox vcard\")\n",
    "\n",
    "        if table is not None:\n",
    "            table_rows = table.findAll('tr')\n",
    "            for row in table_rows:\n",
    "                feature = row.find('th')\n",
    "                if feature is not None:\n",
    "                    feature = feature.text\n",
    "                    features.append(feature)\n",
    "    return pd.Series(features).unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the list of features we can potentially extract about a INGO\n",
    "list_features(INGO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the features displayed above, we can handcraft lists of more or less synonymous features that will help us create a function capable of handling different type of wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_features = ['CEO', 'Secretary General', 'Owner', 'Key people',\n",
    "                                'Chair', 'Co-executive director', 'President', 'Board Chair',\n",
    "                                'Chief Executive Officer', 'Board of Directors', 'Executive Director',\n",
    "                                'National President & CEO', 'Chairman', 'Chief Executive', \n",
    "                                'Deputy Secretary General', 'Chair of the Supervisory Board',\n",
    "                                'Chairman of the Governing Body', 'President/CEO', 'President and CEO',\n",
    "                                'Board\\xa0of directors', 'President of the Board', 'Board of Trustees',\n",
    "                                'Chair, Adult Advisory Council', 'Leader', 'President, Treasurer', \n",
    "                                'President & CEO', 'Executive director', 'Vice president', 'Predecessor',\n",
    "                                'Chairman of Governors', 'Notable Board Members[1]', 'Board of Directors',\n",
    "                                'Superior General', 'Honorary President', 'Co-Chairman', 'Managing Director',\n",
    "                                'Chair World Board', 'President Emeritus', 'Director General',\n",
    "                                'Founder', 'Founders']\n",
    "\n",
    "money_features = ['Revenue', 'Revenue  ',\n",
    "                                'Revenue .mw-parser-output .nobold{font-weight:normal}(2016) ',\n",
    "                                'Revenue .mw-parser-output .nobold{font-weight:normal}(2015) ', \n",
    "                                'Revenue .mw-parser-output .nobold{font-weight:normal}(2014) ',\n",
    "                                'Revenue .mw-parser-output .nobold{font-weight:normal}(2017) ',\n",
    "                                'Revenue .mw-parser-output .nobold{font-weight:normal}(2016) ',\n",
    "                                'Net income', 'Total assets', 'Operating income',\n",
    "                                'Revenue (FY 2015)' ]\n",
    "\n",
    "hq_features = ['Headquarters', 'Address']\n",
    "\n",
    "country_features = ['Location', 'Chapters', 'Region ', 'Country']\n",
    "\n",
    "old_name_features = ['Formerly called', 'Abbreviation', 'Parent organization']\n",
    "\n",
    "affiliation_features = ['Subsidiaries', 'Affiliation']\n",
    "\n",
    "purpose_features = ['Purpose', 'Focus', 'Product', 'Services', 'Industry', 'Fields']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get down to work and extract actual information from the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to extract information from a wikipedia page about a INGO\n",
    "\n",
    "#using the list above, go through the INGO wikipedia pages and attempt to draw out any possible information\n",
    "def extract_info_from_table(table):\n",
    "    '''\n",
    "    Extracts information from a given html table\n",
    "    \n",
    "    Input: an html table containing information about a charity\n",
    "    Output: list of string of the following shape about a charity (if present):\n",
    "        [leader1, leader2, leader3, revenue, hq, location, other_names, subsidiaries, purpose]\n",
    "    '''\n",
    "    \n",
    "    #placeholders for the information we want\n",
    "    leader1 = None\n",
    "    leader2 = None\n",
    "    leader3 = None\n",
    "    leader4 = None\n",
    "    leader1_set = False\n",
    "    leader2_set = False\n",
    "    leader3_set = False\n",
    "    revenue = None\n",
    "    hq = None\n",
    "    location = None\n",
    "    other_names = None\n",
    "    subsidiaries = None\n",
    "    purpose = None\n",
    "        \n",
    "    if table is not None:\n",
    "        #for each table row\n",
    "        table_rows = table.findAll('tr')\n",
    "        for row in table_rows:\n",
    "            #find the feature and its value\n",
    "            feature = row.find('th')\n",
    "            value = row.find('td')\n",
    "            if feature is not None and value is not None:\n",
    "                #cast them to strings\n",
    "                feature = feature.text\n",
    "                value = value.text\n",
    "                \n",
    "                #check which category the feature belongs to and insert the value in a placeholder\n",
    "                if feature in people_features:\n",
    "                    if leader1_set == False:\n",
    "                        leader1 = value\n",
    "                        leader1_set = True\n",
    "                    elif leader2_set == False:\n",
    "                        leader2 = value\n",
    "                        leader2_set = True\n",
    "                    elif leader3_set == False:\n",
    "                        leader3 = value\n",
    "                        leader3_set = True\n",
    "                    elif leader4_set == False:\n",
    "                        leader4 = value\n",
    "                        leader4_set = True\n",
    "                elif feature in money_features:\n",
    "                    revenue = value\n",
    "                elif feature in hq_features:\n",
    "                    hq = value\n",
    "                elif feature in country_features:\n",
    "                    location = value\n",
    "                elif feature in old_name_features:\n",
    "                    other_names = value\n",
    "                elif feature in affiliation_features:\n",
    "                    subsidiaries = value\n",
    "                elif feature in purpose_features:\n",
    "                    purpose = value\n",
    "                    \n",
    "    #return the information\n",
    "    return [leader1, leader2, leader3, revenue, hq, location, other_names, subsidiaries, purpose]\n",
    "    \n",
    "        \n",
    "\n",
    "def extract_info_from_link(link):\n",
    "    '''\n",
    "    Given a wikipedia link, find its navbox and extract information\n",
    "    \n",
    "    Input: wikipedia link\n",
    "    Output: list of string of the following shape about a charity (if present):\n",
    "            [leader1, leader2, leader3, revenue, hq, location, other_names, subsidiaries, purpose]\n",
    "    \n",
    "    '''\n",
    "    r_link = requests.get(link)\n",
    "    soup_link = BeautifulSoup(r_link.text, 'html.parser')\n",
    "    table = soup_link.find('table', class_=\"infobox vcard\")\n",
    "    return extract_info_from_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these helper functions, we iterate through the links and check each one for information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of INGO information\n",
    "detailed_INGO = []\n",
    "for index, row in INGO.iterrows():\n",
    "    name = row['Name']\n",
    "    link = row['Link']\n",
    "    INGO_details = extract_info_from_link(link)\n",
    "    INGO_details.insert(0, name)\n",
    "    detailed_INGO.append(INGO_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned and renamed dataframe categories\n",
    "INGO_detailed = pd.DataFrame(detailed_INGO,\n",
    "                                  columns=['Name', 'Leader 1', 'Leader 2', 'Leader 3', 'Leader 4',\n",
    "                                           'Revenue', 'Headquarters', 'Location', 'Other names', 'Subsidiaries',\n",
    "                                          'Purpose'])\n",
    "\n",
    "INGO_detailed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INGO_detailed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is still relatively messy, with lots of information missing, and sometimes misstored columns or mulitple values in each field. This is okay, though, because the names are neat and they will be our primary point of comparison with the panama papers. Once we have narrowed down the charities based on the names, if we need more information about the (hopefully) small subset of charities, we can do more cleaning then. \n",
    "\n",
    "Meanwhile, let's save this information and analyse it a bit more carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data as a csv file\n",
    "INGO_detailed.to_csv('../../generated/INGO/wikipedia_INGO_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload the data to make sure it's okay\n",
    "df = pd.read_csv('../../generated/INGO/wikipedia_INGO_info.csv', index_col=0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it survived the csv. Time to analyse!\n",
    "\n",
    "We start with the names, since they are the most important part of the charity for our research (primary point of comparison with the panama papers).\n",
    "\n",
    "To figure out how often we might accidentally come across matches based on rather generic names, we analyse the words that make up the names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of extracted INGO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df['Name']\n",
    "\n",
    "#Separate the words in the names\n",
    "split_names = names.str.split()\n",
    "words_in_names = pd.DataFrame([word for name in split_names for word in name], columns=['Words'])\n",
    "\n",
    "#Count how often specific words appear\n",
    "words_in_names['Count'] = 1\n",
    "word_frequency = words_in_names.groupby('Words').count().reset_index()\\\n",
    "                    .sort_values(by='Count', ascending=False)\\\n",
    "                    .reset_index(drop=True)\n",
    "word_frequency.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a lot of \"stop words\" (words that exist mostly for grammatical purposes, like \"of\" and \"for\"). Since these aren't very interesting (because they are everywhere!) we can remove them using nltk's predefined list of English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "big_words = word_frequency[word_frequency['Words'].str.lower().isin(stop_words) == False]\\\n",
    "                    .sort_values(by='Count', ascending=False)\\\n",
    "                    .reset_index(drop=True)\n",
    "big_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_words.plot(title='Distribution of the number of times a word appears in INGO names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_words.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of our approximate 85 words, we seem to have quite a few  that are used multiple times. Let's find out what they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = big_words[big_words['Count'] > 2]\n",
    "common_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words.plot(kind='barh', x='Words', y='Count', figsize=(15, 10),\n",
    "                  title='Words appearing more than twice in names, by frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while there are a few \"generic\" words (International, World, Federation) that are not suprising for INGOs, some of the them are rather specialized (Youth, Nature, Children). These are the ones that should help us track down charities in the panama papers or other leaked data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
