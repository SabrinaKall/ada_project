{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs\n",
    "\n",
    "This notebook is similar to the second part of the __Analysis notebook__. Its sole purpose is to create the same graphs as before, but in a format that can be used for the data story's moving network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n",
    "from community import community_louvain\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use degree 1, as larger degrees resulted in unmanagably large graphs. But distance 1 is quite good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables \n",
    "\n",
    "DEGREE = 1\n",
    "\n",
    "DEGREE_FILE = '../generated/map/degree_' + str(DEGREE) +'/'\n",
    "\n",
    "def get_graph_elem_file(elem_type, leak):\n",
    "    return DEGREE_FILE + leak + '_' + elem_type + '.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the graphs\n",
    "\n",
    "Note that as some of the graphs have still very many nodes, we can choose to filter these down to only those connected to more than one node (unless a node is a match -- we still want to know who is not connected to anyone).\n",
    "\n",
    "Note the special attention drawn to the \"bahamas\" leak, which has a slightly different schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_loners(full_df, clusters_only, cluster_size=2):\n",
    "    \n",
    "    '''Removes nodes with fewer than 2 connections who are not matches'''\n",
    "    \n",
    "    node_counts = full_df['START_ID'].append(full_df['END_ID']).value_counts()\n",
    "\n",
    "    ids_to_remove = []\n",
    "        \n",
    "    for index, row in full_df.iterrows():\n",
    "        start_id = row['START_ID']\n",
    "        end_id = row['END_ID']\n",
    "        \n",
    "        if clusters_only:\n",
    "            if ((node_counts[start_id] < cluster_size) and (node_counts[end_id] < cluster_size)):\n",
    "                ids_to_remove.append(index)\n",
    "            \n",
    "        else:\n",
    "            if ((row['match_x'] == False and node_counts[start_id] < cluster_size)\n",
    "                or (row['match_y'] == False and node_counts[end_id] < cluster_size)):\n",
    "                ids_to_remove.append(index)\n",
    "    \n",
    "    return full_df.drop(full_df.index[ids_to_remove]).reset_index(drop = True)\n",
    "\n",
    "#The moving network needs node ids starting at 0, so we reset them\n",
    "def zero_id_nodes(full_df):\n",
    "    '''Sets the indexes of the nodes to zero (needed for the graph)'''\n",
    "    id_map = {}\n",
    "    curr_id = 0\n",
    "    \n",
    "    for index, row in full_df.iterrows():\n",
    "        start_id = row['START_ID']\n",
    "        end_id = row['END_ID']\n",
    "        \n",
    "        if start_id not in id_map:\n",
    "            id_map[start_id] = curr_id\n",
    "            curr_id +=1\n",
    "            \n",
    "        if end_id not in id_map:\n",
    "            id_map[end_id] = curr_id\n",
    "            curr_id +=1\n",
    "            \n",
    "        full_df.at[index, 'START_ID'] = id_map[start_id]\n",
    "        full_df.at[index, 'END_ID'] = id_map[end_id]\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "def extract_nodes(full_df):\n",
    "    start_nodes = full_df[['START_ID', 'name_x', 'match_x']]\n",
    "    end_nodes = full_df[['END_ID', 'name_y', 'match_y']]\n",
    "    \n",
    "    start_nodes.rename(columns={'START_ID':'id', 'name_x': 'name', 'match_x': 'match'}, inplace = True)\n",
    "    end_nodes.rename(columns={'END_ID':'id', 'name_y': 'name', 'match_y': 'match'}, inplace = True)\n",
    "    \n",
    "    nodes = start_nodes.append(end_nodes).drop_duplicates(['id', 'name', 'match'])\n",
    "    \n",
    "    return nodes\n",
    "    \n",
    "    \n",
    "\n",
    "def graph_leak_matches(leak, dense = False, clusters_only = False, cluster_size = 2):\n",
    "    \n",
    "    '''Creates graphs of matches found in a given leak'''\n",
    "    \n",
    "    bahamas = (leak == 'bahamas')\n",
    "\n",
    "    node_id = '0'\n",
    "    name_index = '1'\n",
    "\n",
    "    if bahamas:\n",
    "        node_id = '4' \n",
    "        name_index = '7'\n",
    "    \n",
    "    #Load the data\n",
    "    nodes = pd.read_csv(get_graph_elem_file('nodes', leak), index_col = 0)\\\n",
    "                                                            .reset_index(drop = True)[[node_id, name_index, 'Match']]\n",
    "\n",
    "    nodes.rename(columns={node_id:'node_id', name_index:'name', 'Match':'match'}, inplace = True)\n",
    "    edges = pd.read_csv(get_graph_elem_file('edges', leak), index_col = 0).reset_index(drop = True)[['START_ID', 'END_ID']]\n",
    "\n",
    "    #Format the data as a set of edges with information about the nodes\n",
    "    full_df = pd.merge(nodes, edges, left_on = 'node_id', right_on = 'START_ID')\n",
    "    full_df = pd.merge(full_df, nodes, left_on = 'END_ID', right_on = 'node_id').drop(['node_id_x', 'node_id_y'], axis = 1)\n",
    "    \n",
    "    #filter too large graphs if told so\n",
    "    if dense:\n",
    "        print('Size before removing loners: ' + str(len(full_df)))\n",
    "        full_df = remove_loners(full_df, clusters_only, cluster_size)\n",
    "        print('Size after removing loners: ' + str(len(full_df)))\n",
    "        \n",
    "    full_df = zero_id_nodes(full_df)\n",
    "    \n",
    "    graph_nodes = extract_nodes(full_df)\n",
    "    \n",
    "    #create the graph\n",
    "    graph = nx.DiGraph()\n",
    "    for i in range(0,len(full_df)):\n",
    "        graph.add_edge(int(full_df.loc[i,\"START_ID\"]), int(full_df.loc[i,'END_ID']))\n",
    "\n",
    "    for n in graph:\n",
    "        graph.node[n]['name'] = graph_nodes.loc[graph_nodes['id'] == n, 'name'].to_string(index=False)\n",
    "        graph.node[n]['match'] = graph_nodes.loc[graph_nodes['id'] == n, 'match'].bool()\n",
    "        \n",
    "            \n",
    "    \n",
    "    #export as .json\n",
    "    d = json_graph.node_link_data(graph)\n",
    "    name = leak\n",
    "    \n",
    "    if dense:\n",
    "        name = leak + \"_small\"\n",
    "    json.dump(d, open('../results/graphs/graph_'+ name +'.json','w'))\n",
    "    \n",
    "    graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('paradise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before removing loners: 234\n",
      "Size after removing loners: 57\n"
     ]
    }
   ],
   "source": [
    "graph_leak_matches('paradise', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('offshore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before removing loners: 98\n",
      "Size after removing loners: 33\n"
     ]
    }
   ],
   "source": [
    "graph_leak_matches('offshore', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('bahamas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
