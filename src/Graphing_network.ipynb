{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs\n",
    "\n",
    "Using the previously collected data detailing how matches connect to the rest of the leaked information, we can create graphs and see the degree of interconnectivity between the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import json\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter\n",
    "from community import community_louvain\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use degree 1, as larger degrees resulted in unmanagably large graphs. But distance 1 is quite good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables \n",
    "\n",
    "DEGREE = 1\n",
    "\n",
    "DEGREE_FILE = '../generated/map/degree_' + str(DEGREE) +'/'\n",
    "\n",
    "def get_graph_elem_file(elem_type, leak):\n",
    "    return DEGREE_FILE + leak + '_' + elem_type + '.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the graphs\n",
    "\n",
    "Note that as some of the graphs have still very many nodes, we can choose to filter these down to only those connected to more than one node (unless a node is a match -- we still want to know who is not connected to anyone).\n",
    "\n",
    "Note the special attention drawn to the \"bahamas\" leak, which has a slightly different schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nan(full_df, edge_df, node_df):\n",
    "    \n",
    "    '''Removes nodes NaN names'''\n",
    "\n",
    "    ids_to_remove = []\n",
    "\n",
    "    full_df_error = full_df.replace(np.nan, 'ERROR', regex=True)\n",
    "    \n",
    "    for index, row in full_df_error.iterrows():\n",
    "        start_id = row['START_ID']\n",
    "        end_id = row['END_ID']\n",
    "        \n",
    "        if row['name_x'] == 'ERROR':\n",
    "            ids_to_remove.append(start_id)\n",
    "        if row['name_y'] == 'ERROR':\n",
    "            ids_to_remove.append(end_id)\n",
    "    \n",
    "    print(ids_to_remove)\n",
    "    \n",
    "    edge_clean = edge_df[~edge_df['START_ID'].isin(ids_to_remove)]\n",
    "    edge_clean = edge_clean[~edge_clean['END_ID'].isin(ids_to_remove)]\n",
    "    \n",
    "    node_clean = node_df[node_df['node_id'].isin(edge_clean['START_ID']) | node_df['node_id'].isin(edge_clean['END_ID'])]\n",
    "    \n",
    "    edge_clean = edge_clean.reset_index(drop=True)\n",
    "    node_clean = node_clean.reset_index(drop=True)\n",
    "    node_clean['id'] = node_clean.index\n",
    "        \n",
    "    full_df_clean = pd.merge(node_clean, edge_clean, left_on='node_id', right_on='START_ID')\n",
    "    full_df_clean = pd.merge(full_df_clean, node_clean, left_on='END_ID', right_on='node_id').drop(['node_id_x', 'node_id_y'], axis=1)\n",
    "        \n",
    "    return full_df_clean, edge_clean, node_clean\n",
    "\n",
    "def remove_loners(full_df, edge_df, node_df):\n",
    "    \n",
    "    '''Removes nodes with fewer than 2 connections who are not matches'''\n",
    "    \n",
    "    node_counts = full_df['START_ID'].append(full_df['END_ID']).value_counts()\n",
    "\n",
    "    ids_to_remove = []\n",
    "\n",
    "    for index, row in full_df.iterrows():\n",
    "        start_id = row['START_ID']\n",
    "        end_id = row['END_ID']\n",
    "        \n",
    "        if (row['match_x'] == False and node_counts[start_id] < 2):\n",
    "            ids_to_remove.append(start_id)\n",
    "        if (row['match_y'] == False and node_counts[end_id] < 2):\n",
    "            ids_to_remove.append(end_id)\n",
    "    \n",
    "    edge_lean = edge_df[~edge_df['START_ID'].isin(ids_to_remove)]\n",
    "    edge_lean = edge_lean[~edge_lean['END_ID'].isin(ids_to_remove)]\n",
    "    \n",
    "    node_lean = node_df[node_df['node_id'].isin(edge_lean['START_ID']) | node_df['node_id'].isin(edge_lean['END_ID'])]\n",
    "    return edge_lean, node_lean\n",
    "\n",
    "def graph_leak_matches(leak, dense=False):\n",
    "    \n",
    "    '''Creates graphs of matches found in a given leak'''\n",
    "    \n",
    "    bahamas = (leak == 'bahamas')\n",
    "\n",
    "    node_id = '0'\n",
    "    name_index = '1'\n",
    "\n",
    "    if bahamas:\n",
    "        node_id = '4' \n",
    "        name_index = '7'\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (23,23)\n",
    "\n",
    "    \n",
    "    #Load the data\n",
    "    nodes = pd.read_csv(get_graph_elem_file('nodes', leak), index_col=0)\\\n",
    "                                                            .reset_index(drop=True)[[node_id, name_index, 'Match']]\n",
    "\n",
    "    nodes.rename(columns={node_id:'node_id', name_index:'name', 'Match':'match'}, inplace=True)\n",
    "    nodes['id'] = nodes.index\n",
    "    \n",
    "    edges = pd.read_csv(get_graph_elem_file('edges', leak), index_col=0).reset_index(drop=True)[['START_ID', 'END_ID']]\n",
    "    \n",
    "    #Format the data as a set of edges with information about the nodes\n",
    "    full_df = pd.merge(nodes, edges, left_on='node_id', right_on='START_ID')\n",
    "    full_df = pd.merge(full_df, nodes, left_on='END_ID', right_on='node_id').drop(['node_id_x', 'node_id_y'], axis=1)\n",
    "    \n",
    "    #filter too large graphs if told so\n",
    "    if dense:\n",
    "        print('Size before removing loners: ' + str(len(full_df)))\n",
    "        edges_lean, nodes_lean = remove_loners(full_df, edges, nodes)\n",
    "        edges_lean = edges_lean.reset_index(drop=True)\n",
    "        nodes_lean = nodes_lean.reset_index(drop=True)\n",
    "        nodes_lean['id'] = nodes_lean.index\n",
    "        \n",
    "        full_df_lean = pd.merge(nodes_lean, edges_lean, left_on='node_id', right_on='START_ID')\n",
    "        full_df_lean = pd.merge(full_df_lean, nodes_lean, left_on='END_ID', right_on='node_id').drop(['node_id_x', 'node_id_y'], axis=1)\n",
    "        \n",
    "        nodes = nodes_lean\n",
    "        edges = edges_lean\n",
    "        full_df = full_df_lean\n",
    "        print('Size after removing loners: ' + str(len(full_df)))\n",
    "        \n",
    "    #prepare for coloring the nodes according to if it is a match to a charity or not\n",
    "    match_class_1 = full_df[['name_x', 'match_x']].rename(columns={'name_x':'name', 'match_x':'match'})\n",
    "    match_class_2 = full_df[['name_y', 'match_y']].rename(columns={'name_y':'name', 'match_y':'match'})\n",
    "    \n",
    "    match_class = match_class_1.append(match_class_2).set_index('name')\n",
    "    match_class = match_class[~match_class.index.duplicated(keep='first')]\n",
    "    \n",
    "    #match_class = match_class.reindex(graph.nodes())\n",
    "    #match_class['match'] = pd.Categorical(match_class['match'])\n",
    "    \n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    for i in range(0,len(full_df)):\n",
    "        graph.add_edge(int(full_df.loc[i,\"id_x\"]), int(full_df.loc[i,'id_y']))\n",
    "    \n",
    "    if dense:\n",
    "        for n in graph:\n",
    "            graph.node[n]['name'] = nodes_lean.loc[nodes_lean['id'] == n, 'name'].to_string(index=False)\n",
    "            graph.node[n]['match'] = nodes_lean.loc[nodes_lean['id'] == n, 'match'].bool()\n",
    "    else:\n",
    "        for n in graph:\n",
    "            graph.node[n]['name'] = nodes.loc[nodes['id'] == n, 'name'].to_string(index=False)\n",
    "            graph.node[n]['match'] = nodes.loc[nodes['id'] == n, 'match'].bool()\n",
    "    \n",
    "    #export as .json\n",
    "    d = json_graph.node_link_data(graph)\n",
    "    json.dump(d, open('network/force.json','w'))\n",
    "    \n",
    "    print('Matches for ' + leak + ' papers: ')\n",
    "    print(nx.info(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before removing loners: 37\n",
      "Size after removing loners: 11\n",
      "Matches for panama papers: \n",
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 14\n",
      "Number of edges: 9\n",
      "Average in degree:   0.6429\n",
      "Average out degree:   0.6429\n"
     ]
    }
   ],
   "source": [
    "graph_leak_matches('panama', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
