{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis\n",
    "\n",
    "Now that we have cleaned, filtered and preprocessed our data, we can proceed to analyse it to see whether any conclusions can be drawn from the found matches.\n",
    "\n",
    "This will be done in two parts:\n",
    "\n",
    "First, we check whether any of the addresses, if present, of the shell companies is near the headquarter location of the charity it matches against.\n",
    "\n",
    "Secondly, we will create graphs of all the found shell companies to analyse their distribution in the leaked papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note that in order to connect nodes to each other, both these points rest on substantial preprocessing done in the \"Connections\" notebook, which can be consulted for a better understanding of the structures being dealt with.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "\n",
    "#Spark\n",
    "import findspark\n",
    "findspark.init(r\"C:\\Users\\Lucas\\Desktop\\ADA\\spark-2.3.2-bin-hadoop2.7\")\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "#imports\n",
    "import networkx as nx\n",
    "\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "from community import community_louvain\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addresses\n",
    "\n",
    "Our goal is to find addresses connected to shell companies and see if the match the headquarters of the charity. This could be seen as a red flag that the shell company indeed represents the charity it was matched with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can reuse the filtering functions from earlier, this time looking specifically for edges of type \"address\" connected\n",
    "#to nodes of type entity\n",
    "\n",
    "def filter_edges(edges, nodes):\n",
    "    '''Given a set of nodes, returns the edges connected to those nodes'''\n",
    "    \n",
    "    entity_ids = nodes.map(lambda r: r[0]).collect()\n",
    "    \n",
    "    return edges.filter(lambda r: r[0] in entity_ids or r[2] in entity_ids)\n",
    "\n",
    "def filter_nodes(nodes, edges):\n",
    "    '''Given a set of edges, return the nodes connected to those edges'''\n",
    "    \n",
    "    start_ids = edges.map(lambda r: r[0]).collect()\n",
    "   \n",
    "    end_ids = edges.map(lambda r: r[2]).collect()\n",
    "    \n",
    "    return nodes.rdd.filter(lambda r: r[0] in start_ids or r[0] in end_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_matches(leak):\n",
    "    '''return the match node id of first level connection with registered address '''\n",
    "    \n",
    "    #return the path of edges csv file\n",
    "    edges = spark.read.csv('../data/'+ leak + '/*.edges.csv', header=True)\n",
    "    \n",
    "    #return the path of address nodes csv file\n",
    "    address_nodes = spark.read.csv('../data/' + leak + '/*address.csv', header=True)\n",
    "    \n",
    "    #filter the edges that contain a registered address\n",
    "    address_edges = edges.rdd.filter(lambda r: r[1] == 'registered_address')\n",
    "    \n",
    "    #get the matches \n",
    "    matches = spark.read.csv('../generated/matches/entity_matches.csv')\n",
    "    \n",
    "    #filter the edges connected to matches\n",
    "    entity_address_edges = filter_edges(address_edges, matches.rdd)\n",
    "    \n",
    "    #filter thes nodes linked to these edges\n",
    "    leak_address = filter_nodes(address_nodes,entity_address_edges)\n",
    "    \n",
    "    #create a dataframe from it\n",
    "    address = spark.createDataFrame(leak_address)\n",
    "    \n",
    "    ''' Conversion of the spark dataframe in pandas ones : '''\n",
    "    \n",
    "    #for the matches :\n",
    "    matches_pd = pd.DataFrame(matches.collect(), columns=[\"node_id\",\"ShellName\",\"CharityName\",\"CharityHeadquarters\"])\n",
    "    \n",
    "    #for the nodes connected to the matches :\n",
    "    address_pd = pd.DataFrame(address.collect(),\n",
    "                            columns = [\"node_id\",\"name\",\"address\",\"country_codes\",\n",
    "                                       \"countries\",\"sourceID\",\"valid_until\",\"note\"])\n",
    "    \n",
    "    #for the edges connected to the matches :\n",
    "    entity_address_edges_pd = pd.DataFrame(entity_address_edges.collect(),\n",
    "                            columns = [\"node_id\",\"TYPE\",\"END_ID\",\"link\",\"start_date\",\n",
    "                                       \"end_date\",\"sourceID\",\"valid_until\"])\n",
    "    \n",
    "    '''Apply 'int' type to the columns used for the merge : '''\n",
    "    \n",
    "    matches_pd['node_id'] = matches_pd['node_id'].apply(int)\n",
    "   \n",
    "    address_pd['node_id'] = address_pd['node_id'].apply(int)\n",
    "    \n",
    "    entity_address_edges_pd['node_id']=entity_address_edges_pd['node_id'].apply(int)\n",
    "    \n",
    "    #rename the column adequatly for merging\n",
    "    address_pd.rename(columns = {'node_id':'END_ID'}, inplace=True)\n",
    "    \n",
    "    #drop useless columns in order to avoid conflict when merging\n",
    "    address_pd.drop(['note','valid_until','sourceID'],axis = 1, inplace = True)\n",
    "    \n",
    "    #first we merge in order to obtain correspondance between the matches and \n",
    "    #the corresponding edges with registered adress\n",
    "    maches_edges = matches_pd.merge(entity_address_edges_pd, on = 'node_id')\n",
    "    \n",
    "    maches_edges['END_ID'] = maches_edges['END_ID'].apply(int)\n",
    "    \n",
    "    #then to node linked to these edges\n",
    "    matches_nodes = maches_edges.merge(address_pd, on = 'END_ID')\n",
    "\n",
    "    return matches_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = ['panama', 'paradise', 'bahamas', 'offshore']\n",
    "\n",
    "address_matches = []\n",
    "for leak_name in leaks:\n",
    "    #Look for first level connection with registered address in Paradise dataset\n",
    "    leak_addresses = get_address_matches(leak_name)\n",
    "    print('The ' + leak_name + 'papers have' + leak_addresses.count() + 'connections with registered addresses.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we want to compare these addresses with the addresses of the actual charity headquarters\n",
    "paradise.dropna(subset=['CharityHeadquarters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for first level connection with registered address in Panamas dataset\n",
    "panama = get_address_matches('panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for first level connection with registered address in Offshore dataset\n",
    "offshore = get_address_matches('offshore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "Using the previously collected data detailing how matches connect to the rest of the leaked information, we can create graphs and see the degree of interconnectivity between the matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only use degree 1, as larger degrees resulted in unmanagably large graphs. But distance 1 is quite good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables \n",
    "\n",
    "DEGREE = 1\n",
    "\n",
    "DEGREE_FILE = '../generated/map/degree_' + str(DEGREE) +'/'\n",
    "\n",
    "def get_graph_elem_file(elem_type, leak):\n",
    "    return DEGREE_FILE + leak + '_' + elem_type + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the graphs\n",
    "\n",
    "Note that as some of the graphs have still very many nodes, we can choose to filter these down to only those connected to more than one node (unless a node is a match -- we still want to know who is not connected to anyone).\n",
    "\n",
    "Note the special attention drawn to the \"bahamas\" leak, which has a slightly different schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_loners(full_df):\n",
    "    \n",
    "    '''Removes nodes with fewer than 2 connections who are not matches'''\n",
    "    \n",
    "    node_counts = full_df['START_ID'].append(full_df['END_ID']).value_counts()\n",
    "\n",
    "    ids_to_remove = []\n",
    "\n",
    "    for index, row in full_df.iterrows():\n",
    "        start_id = row['START_ID']\n",
    "        end_id = row['END_ID']\n",
    "\n",
    "        if (row['match_x'] == (False and node_counts[start_id] < 2) \n",
    "            or (row['match_y'] == False and node_counts[end_id] < 2)):\n",
    "            ids_to_remove.append(index)\n",
    "    \n",
    "    return full_df.drop(full_df.index[ids_to_remove]).reset_index(drop=True)\n",
    "\n",
    "def graph_leak_matches(leak, dense=False):\n",
    "    \n",
    "    '''Creates graphs of matches found in a given leak'''\n",
    "    \n",
    "    bahamas = (leak == 'bahamas')\n",
    "\n",
    "    node_id = '0'\n",
    "    name_index = '1'\n",
    "\n",
    "    if bahamas:\n",
    "        node_id = '4' \n",
    "        name_index = '7'\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (23,23)\n",
    "\n",
    "    \n",
    "    #Load the data\n",
    "    nodes = pd.read_csv(get_graph_elem_file('nodes', leak), index_col=0)\\\n",
    "                                                            .reset_index(drop=True)[[node_id, name_index, 'Match']]\n",
    "\n",
    "    nodes.rename(columns={node_id:'node_id', name_index:'name', 'Match':'match'}, inplace=True)\n",
    "    edges = pd.read_csv(get_graph_elem_file('edges', leak), index_col=0).reset_index(drop=True)[['START_ID', 'END_ID']]\n",
    "\n",
    "    #Format the data as a set of edges with information about the nodes\n",
    "    full_df = pd.merge(nodes, edges, left_on='node_id', right_on='START_ID')\n",
    "    full_df = pd.merge(full_df, nodes, left_on='END_ID', right_on='node_id').drop(['node_id_x', 'node_id_y'], axis=1)\n",
    "    \n",
    "    #filter too large graphs if told so\n",
    "    if dense:\n",
    "        print('Size before removing loners: ' + str(len(full_df)))\n",
    "        full_df = remove_loners(full_df)\n",
    "        print('Size after removing loners: ' + str(len(full_df)))\n",
    "    \n",
    "    #create the graph\n",
    "    graph = nx.from_pandas_edgelist(full_df, 'name_x', 'name_y', edge_attr=None, create_using= nx.Graph())\n",
    "    \n",
    "    #prepare for coloring the nodes according to if it is a match to a charity or not\n",
    "    match_class_1 = full_df[['name_x', 'match_x']].rename(columns={'name_x':'name', 'match_x':'match'})\n",
    "    match_class_2 = full_df[['name_y', 'match_y']].rename(columns={'name_y':'name', 'match_y':'match'})\n",
    "    \n",
    "    match_class = match_class_1.append(match_class_2).set_index('name')\n",
    "    match_class = match_class[~match_class.index.duplicated(keep='first')]\n",
    "    \n",
    "    match_class = match_class.reindex(graph.nodes())\n",
    "    match_class['match'] = pd.Categorical(match_class['match'])\n",
    "    \n",
    "    print('Matches for ' + leak + ' papers: ')\n",
    "    print(nx.info(graph))\n",
    "    \n",
    "    #draw the graph\n",
    "    nx.draw(graph, nx.spring_layout(graph, scale=60, k=0.25), with_labels=True, alpha=0.7, node_size=1000,\n",
    "        node_color=match_class['match'].cat.codes, cmap=plt.cm.autumn)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display graphs\n",
    "\n",
    "We display two graphs for each set of leaks, one with all the nodes, and a smaller, denser one with only nodes belonging to clusters of a certain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('panama', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('paradise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('paradise', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('offshore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('offshore', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('bahamas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_leak_matches('bahamas', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
