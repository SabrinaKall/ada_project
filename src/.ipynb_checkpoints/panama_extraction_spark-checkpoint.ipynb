{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to find scraped charity names in the Panama Papers using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8529c7babe88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/opt/spark/spark-2.3.2-bin-hadoop2.7/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\ada\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#stop words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "findspark.init(r'C:\\Users\\Ruijia\\Spark')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Addition of english stop words\n",
    "\n",
    "def init_stopwords():\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    stop_words.add('&')\n",
    "    stop_words.add('co')\n",
    "    stop_words.add('co.')\n",
    "    stop_words.add('co.,')\n",
    "    stop_words.add('co.,ltd.')\n",
    "    stop_words.add('corp')\n",
    "    stop_words.add('corp.')\n",
    "    stop_words.add('corp.,')\n",
    "    stop_words.add('de')\n",
    "    stop_words.add('foundation')\n",
    "    stop_words.add('inc')\n",
    "    stop_words.add('inc.')\n",
    "    stop_words.add('limited')\n",
    "    stop_words.add('international')\n",
    "    stop_words.add('ltd')\n",
    "    stop_words.add('ltd.')\n",
    "    stop_words.add('s.a.')\n",
    "    stop_words.add('world')\n",
    "    stop_words.add('global')\n",
    "\n",
    "    stop_words = list(stop_words)\n",
    "    return stop_words\n",
    "\n",
    "stop_words = init_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_words(charity, shell, stop_words, tuning):\n",
    "    \n",
    "    percentage = 0.6\n",
    "    \n",
    "    if charity is None or shell is None:\n",
    "        return False\n",
    "    \n",
    "    charity_words = [x.lower() for x in charity.split()]\n",
    "    shell_words = [x.lower() for x in shell.split()]\n",
    "    len_charity = len(charity_words)\n",
    "    len_shell = len(shell_words)\n",
    "    \n",
    "    count_random_matches = 0\n",
    "    stop_word_random_matches = 0\n",
    "    \n",
    "    for i in range(len_charity):\n",
    "        word = charity_words[i]\n",
    "        if word in shell_words:\n",
    "            count_random_matches += 1\n",
    "            \n",
    "            if word in stop_words:\n",
    "                stop_word_random_matches += 1\n",
    "                \n",
    "    if tuning:\n",
    "        #if only stopwords match, not valid\n",
    "        if count_random_matches - stop_word_random_matches < 1:\n",
    "            return False\n",
    "\n",
    "        #\"Family foundations are tricky -> make sure they are not the only matching parts\"\n",
    "        if ('family' in shell_words \n",
    "            and 'foundation' in shell_words \n",
    "            and 'family' in charity_words \n",
    "            and 'foundation' in charity_words \n",
    "            and count_random_matches < 3 \n",
    "            and len_shell > 2 \n",
    "            and len_charity > 2):\n",
    "            return False\n",
    "\n",
    "    if len_charity == 1 or len_shell == 1:\n",
    "        return (np.abs(len_charity - len_shell) < 2  and count_random_matches == 1)\n",
    "        \n",
    "    return ((count_random_matches/len_charity >= percentage) \n",
    "            and (count_random_matches/len_shell >= percentage))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matches_between(leak, charity, sharp):\n",
    "    \n",
    "    stop_words = init_stopwords()\n",
    "    \n",
    "    charity_location = '../generated/' + charity + '/' + charity + '_charity_info.csv'\n",
    "    leak_location = '../data/' + leak + '/' + leak + '*.nodes.entity.csv'\n",
    "    \n",
    "    leak_data = spark.read.csv(leak_location, header=True)\n",
    "\n",
    "    charity_data = spark.read.csv(charity_location, header=True)\n",
    "    \n",
    "    charity_names = charity_data.select('name').selectExpr('name as CharityName')\n",
    "    shell_names = leak_data.select('node_id','name').withColumnRenamed('name', 'ShellName')\n",
    "    \n",
    "    shells_vs_charities = shell_names.crossJoin(charity_names)\n",
    "    \n",
    "    filtered_names = shells_vs_charities.rdd.filter(lambda r: check_for_words(r[1], r[2], stop_words, sharp) == True)\n",
    "    \n",
    "    matches = filtered_names.toDF().toPandas()\n",
    "    \n",
    "    matches.to_csv('../generated/matches/' + leak +'_'+ charity +'_matches.csv')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "extract_matches_between('panama', 'forbes', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "extract_matches_between('panama', 'wikipedia', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "extract_matches_between('panama', 'INGO', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
