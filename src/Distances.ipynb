{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distances\n",
    "\n",
    "Using the matches found for data extraction, find the nodes connected to the matching shell companies up to a given distance using the edges. These will then be stored as csv files and used to create graphs that can visualize the degree of connectivity between the hit nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "findspark.init('/opt/spark/spark-2.3.2-bin-hadoop2.7/')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "DISTANCE_DEGREE = 1\n",
    "\n",
    "def get_match_file(leak, charity, node_type):\n",
    "    return '../generated/inspected_matches/' + node_type +'/' + node_type +'_'+ leak + '_' + charity + '_matches.csv'\n",
    "\n",
    "def get_matches(leak):\n",
    "    \n",
    "    node_types = ['officer', 'entity']\n",
    "    charity_types = ['wikipedia', 'INGO', 'forbes']\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    matches = None\n",
    "    \n",
    "    for node_type in node_types:\n",
    "        for charity_type in charity_types:\n",
    "            \n",
    "            if first:\n",
    "                matches = spark.read.csv(get_match_file(leak, charity_type, node_type), header=True)\n",
    "                first = False\n",
    "            else:\n",
    "                new_matches = spark.read.csv(get_match_file(leak, charity_type, node_type), header=True)\n",
    "                matches.union(new_matches)\n",
    "    \n",
    "    return matches.drop('_c0')\n",
    "    \n",
    "    \n",
    "\n",
    "class Leak_Nodes:\n",
    "    def __init__(self, leak):\n",
    "        self.address_nodes = spark.read.csv('../data/'+ leak +'/'+ leak +'_papers.nodes.address.csv', header=True)\n",
    "        self.intermediary_nodes = spark.read\\\n",
    "                                .csv('../data/'+ leak +'/'+ leak +'_papers.nodes.intermediary.csv', header=True)\n",
    "        self.officer_nodes = spark.read.csv('../data/'+ leak +'/'+ leak +'_papers.nodes.officer.csv', header=True)\n",
    "        self.entity_nodes = spark.read.csv('../data/'+ leak +'/'+ leak +'_papers.nodes.entity.csv', header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter down huge leaks datasets to smaller match datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_edges(edges, nodes):\n",
    "    '''Given a set of nodes, returns the edges connected to those nodes'''\n",
    "    ids = nodes.map(lambda r: r[0]).collect()\n",
    "    return edges.rdd.filter(lambda r: r[0] in ids or r[2] in ids)\n",
    "\n",
    "def filter_nodes(nodes, edges):\n",
    "    '''Given a set of edges, return the nodes connected to those edges'''\n",
    "    start_ids = edges.map(lambda r: r[0]).collect()\n",
    "    end_ids = edges.map(lambda r: r[2]).collect()\n",
    "    \n",
    "    return nodes.rdd.filter(lambda r: r[0] in start_ids or r[0] in end_ids)\n",
    "\n",
    "def get_map_of_degree(degree, matches, leak_nodes, edges):\n",
    "    '''Given a degree, '''\n",
    "    \n",
    "    degree_i_nodes = matches.rdd\n",
    "    \n",
    "    \n",
    "    for i in range(degree):\n",
    "        degree_i_edges = filter_edges(edges, degree_i_nodes)\n",
    "        \n",
    "        degree_i_addresses = filter_nodes(leak_nodes.address_nodes, degree_i_edges)\n",
    "        degree_i_intermediary = filter_nodes(leak_nodes.intermediary_nodes, degree_i_edges)\n",
    "        degree_i_entities = filter_nodes(leak_nodes.entity_nodes, degree_i_edges)\n",
    "        degree_i_officers = filter_nodes(leak_nodes.officer_nodes, degree_i_edges)\n",
    "        \n",
    "        degree_i_nodes = degree_i_addresses\\\n",
    "                                .union(degree_i_intermediary)\\\n",
    "                                .union(degree_i_entities)\\\n",
    "                                .union(degree_i_officers)\n",
    "        \n",
    "    return degree_i_edges, degree_i_nodes\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from(distance_degree, leak):\n",
    "    \n",
    "    edges = spark.read.csv('../data/' + leak +'/' + leak + '_papers.edges.csv', header=True)\n",
    "    \n",
    "    nodes = Leak_Nodes(leak)\n",
    "    \n",
    "    matches = get_matches(leak)\n",
    "    \n",
    "    filtered_edges, filtered_nodes = get_map_of_degree(distance_degree, matches, nodes, edges)\n",
    "    \n",
    "    graph_edges = pd.DataFrame(filtered_edges.collect(),\n",
    "             columns=['START_ID', 'TYPE', 'END_ID', 'link', 'start_date', 'end_date', 'sourceID', 'valid_until'])\n",
    "    graph_nodes = pd.DataFrame(filtered_nodes.collect())\n",
    "    \n",
    "    graph_edges.to_csv('../generated/map/degree_'+ str(distance_degree) +'/'+ leak +'_edges.csv')\n",
    "    graph_nodes.to_csv('../generated/map/degree_'+ str(distance_degree) +'/'+ leak +'_nodes.csv')\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distance_from(1, 'panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_distance_from(1, 'paradise')\n",
    "get_distance_from(1, 'offshore')\n",
    "get_distance_from(1, 'bahamas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
