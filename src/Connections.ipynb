{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connections\n",
    "\n",
    "Using the matches found for data extraction, we find the nodes connected to the matching shell companies up to a given distance using the edges. These will then be stored as csv files and used to create graphs that can visualize the degree of connectivity between the hit nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "findspark.init('/opt/spark/spark-2.3.2-bin-hadoop2.7/')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Until now the matches were spread across several csv files. We group them all in one dataset for easier use.\n",
    "\n",
    "We also create a class for the different types of nodes to make them more portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "\n",
    "DISTANCE_DEGREE = 1\n",
    "\n",
    "def get_match_file(leak, charity, node_type):\n",
    "    return '../generated/inspected_matches/' + node_type +'/' + node_type +'_'+ leak + '_' + charity + '_matches.csv'\n",
    "\n",
    "def get_matches(leak):\n",
    "    \n",
    "    node_types = ['officer', 'entity']\n",
    "    charity_types = ['wikipedia', 'INGO', 'forbes']\n",
    "    \n",
    "    first = True\n",
    "    \n",
    "    matches = None\n",
    "    \n",
    "    for node_type in node_types:\n",
    "        for charity_type in charity_types:\n",
    "            \n",
    "            if first:\n",
    "                matches = spark.read.csv(get_match_file(leak, charity_type, node_type), header=True)\n",
    "                first = False\n",
    "            else:\n",
    "                new_matches = spark.read.csv(get_match_file(leak, charity_type, node_type), header=True)\n",
    "                matches = matches.union(new_matches)\n",
    "                \n",
    "        # save this for later\n",
    "        pd.DataFrame(matches.drop('_c0').collect())\\\n",
    "                            .to_csv('../generated/map/connections/' + node_type +'/'+ node_type +'_' + leak +'_matches.csv')\n",
    "                \n",
    "    matches_clean = matches.drop('_c0')\n",
    "    \n",
    "    #save these in case we need them later\n",
    "    pd.DataFrame(matches_clean.collect()).to_csv('../generated/map/connections/all_'+ leak +'_matches.csv')\n",
    "                \n",
    "                \n",
    "    return matches_clean\n",
    "    \n",
    "    \n",
    "\n",
    "class Leak_Nodes:\n",
    "    def __init__(self, leak):\n",
    "        self.address_nodes = spark.read.csv('../data/'+ leak +'/'+ leak +'*.nodes.address.csv', header=True)\n",
    "        self.intermediary_nodes = spark.read\\\n",
    "                                .csv('../data/'+ leak +'/'+ leak +'*.nodes.intermediary.csv', header=True)\n",
    "        self.officer_nodes = spark.read.csv('../data/'+ leak +'/'+ leak +'*.nodes.officer.csv', header=True)\n",
    "        self.entity_nodes = spark.read.csv('../data/'+ leak +'/'+ leak +'*.nodes.entity.csv', header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering for connectivity\n",
    "\n",
    "In order to find connections, we do the following:\n",
    "\n",
    "    1. Start with the ids of the shells matching charities\n",
    "    2. Filter the edges dataset. Keep the ones expanding from the matches\n",
    "    3. Filter the nodes datasets. Keep the ones at the ends of the edges found in step three and add them to the nodes from step one\n",
    "    4. Repeat steps 2-4 until the satisfactory distance from the start has been reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_edges(edges, nodes):\n",
    "    '''Given a set of nodes, returns the edges connected to those nodes'''\n",
    "    ids = nodes.map(lambda r: r[0]).collect()\n",
    "    return edges.rdd.filter(lambda r: r[0] in ids or r[2] in ids)\n",
    "\n",
    "def filter_nodes(nodes, edges, bahamas):\n",
    "    '''Given a set of edges, return the nodes connected to those edges'''\n",
    "    start_ids = edges.map(lambda r: r[0]).collect()\n",
    "    end_ids = edges.map(lambda r: r[2]).collect()\n",
    "    \n",
    "    index_id = 0\n",
    "    if bahamas:\n",
    "        index_id = 4\n",
    "        \n",
    "    \n",
    "    return nodes.rdd.filter(lambda r: r[index_id] in start_ids or r[index_id] in end_ids)\n",
    "\n",
    "def get_map_of_degree(degree, matches, leak_nodes, edges, bahamas):\n",
    "    '''Given a degree, gets all the nodes connected to matches by degree edges (and the edges too)'''\n",
    "    \n",
    "    degree_i_nodes = matches.rdd\n",
    "    \n",
    "    \n",
    "    for i in range(degree):\n",
    "        degree_i_edges = filter_edges(edges, degree_i_nodes)\n",
    "        \n",
    "        degree_i_addresses = filter_nodes(leak_nodes.address_nodes, degree_i_edges, bahamas)\n",
    "        degree_i_intermediary = filter_nodes(leak_nodes.intermediary_nodes, degree_i_edges, bahamas)\n",
    "        degree_i_entities = filter_nodes(leak_nodes.entity_nodes, degree_i_edges, bahamas)\n",
    "        degree_i_officers = filter_nodes(leak_nodes.officer_nodes, degree_i_edges, bahamas)\n",
    "        \n",
    "        degree_i_nodes = degree_i_addresses\\\n",
    "                                .union(degree_i_intermediary)\\\n",
    "                                .union(degree_i_entities)\\\n",
    "                                .union(degree_i_officers)\n",
    "        \n",
    "    return degree_i_edges, degree_i_nodes\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the filtering to all the datasets\n",
    "\n",
    "Note that the 'bahamas' dataset follows a slightly different schema from the others and requires special treatment. \n",
    "\n",
    "We also mark which nodes were original matches for later use in the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from(distance_degree, leak):\n",
    "    \n",
    "    bahamas = (leak == 'bahamas')\n",
    "    \n",
    "    edges = spark.read.csv('../data/' + leak +'/' + leak + '*.edges.csv', header=True)\n",
    "    \n",
    "    nodes = Leak_Nodes(leak)\n",
    "    \n",
    "    matches = get_matches(leak)\n",
    "    \n",
    "    filtered_edges, filtered_nodes = get_map_of_degree(distance_degree, matches, nodes, edges, bahamas)\n",
    "    \n",
    "    id_index = 0\n",
    "    \n",
    "    if bahamas:\n",
    "        graph_edges = pd.DataFrame(filtered_edges.collect(),\n",
    "             columns=['START_ID', 'TYPE', 'END_ID', 'sourceID', 'valid_until', 'start_date', 'end_date'])\n",
    "        id_index = 4\n",
    "    else:\n",
    "        graph_edges = pd.DataFrame(filtered_edges.collect(),\n",
    "             columns=['START_ID', 'TYPE', 'END_ID', 'link', 'start_date', 'end_date', 'sourceID', 'valid_until'])\n",
    "    \n",
    "    graph_nodes = pd.DataFrame(filtered_nodes.collect())\n",
    "    \n",
    "    match_ids = matches.select('node_id').rdd.flatMap(lambda r: r).collect()\n",
    "    \n",
    "    graph_nodes['Match'] = True\n",
    "    for index, row in graph_nodes.iterrows():\n",
    "        graph_nodes['Match'][index] = (row[id_index] in match_ids)\n",
    "    \n",
    "    graph_edges.to_csv('../generated/map/degree_'+ str(distance_degree) +'/'+ leak +'_edges.csv')\n",
    "    graph_nodes.to_csv('../generated/map/degree_'+ str(distance_degree) +'/'+ leak +'_nodes.csv')\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data for each set of leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distance_from(1, 'panama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distance_from(1, 'paradise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distance_from(1, 'offshore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distance_from(1, 'bahamas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
