{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to find scraped charity names in the Leaked Papers using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#stop words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "#Ruijia\n",
    "#findspark.init(r'C:\\Users\\Ruijia\\Spark')\n",
    "\n",
    "#Sabrina\n",
    "findspark.init('/opt/spark/spark-2.3.2-bin-hadoop2.7/')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word weights for the matching algorithm\n",
    "\n",
    "We use the \"Inverse Document Frequency\" to give words appearing in charity names weights. (Here the names are the documents). This will allow us to reduce the weight of hits based only on very general words (\"foundation x\" is different from \"foundation y\" and should not be marked as a hit, even though 50% of the words in the name are the same!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of word \"foundation\":  1.252762968495368\n",
      "Weight of word \"amnesty\":  6.150602768446279\n",
      "Weight of word \"the\":  2.321961371957184\n"
     ]
    }
   ],
   "source": [
    "#Get names\n",
    "\n",
    "def get_word_weights():\n",
    "    def get_charity_info_location(charity):\n",
    "        return '../generated/' + charity + '/' + charity + '_charity_info.csv'\n",
    "\n",
    "    forbes_names = pd.read_csv(get_charity_info_location('forbes'))['name'].tolist()\n",
    "    wikipedia_names = pd.read_csv(get_charity_info_location('wikipedia'))['name'].tolist()\n",
    "    INGO_names = pd.read_csv(get_charity_info_location('INGO'))['name'].tolist()\n",
    "\n",
    "    names = forbes_names + wikipedia_names + INGO_names\n",
    "\n",
    "    nb_names = len(names)\n",
    "\n",
    "    #Get list of individual words\n",
    "    words = \" \".join(names).split()\n",
    "    nb_words = len(words)\n",
    "\n",
    "    #Get word frequency\n",
    "    word_freq = {} \n",
    "    for word in words: \n",
    "        word = word.lower()\n",
    "        if (word in word_freq): \n",
    "            word_freq[word] += 1\n",
    "        else: \n",
    "            word_freq[word] = 1\n",
    "\n",
    "    #Compute weights: IDF(t) = log_e(Total number of documents (here names) / Number of documents/names with term t in it). \n",
    "    def idf(word_count):\n",
    "        return np.log(nb_names/word_count)\n",
    "\n",
    "    word_weights = {k: idf(v) for k, v in word_freq.items()}\n",
    "    \n",
    "    return word_weights\n",
    "\n",
    "word_weights = get_word_weights()\n",
    "print('Weight of word \"foundation\": ', word_weights.get('foundation'))\n",
    "print('Weight of word \"amnesty\": ', word_weights.get('amnesty'))\n",
    "print('Weight of word \"the\": ', word_weights.get('the'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, very common words, like \"the\" and \"foundation\" have a much weaker weight than rare words like \"amnesty\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note redefined because py4J is interfering with the predefined functions\n",
    "def get_average_weight(word_weights):\n",
    "    average = 0\n",
    "    for weight in word_weights.values(): \n",
    "        average += weight\n",
    "\n",
    "    return average/len(word_weights)\n",
    "\n",
    "def get_min_weight(word_weights):\n",
    "    min_weight = 100\n",
    "    for weight in word_weights.values(): \n",
    "        if weight < min_weight:\n",
    "            min_weight = weight\n",
    "\n",
    "    return min_weight\n",
    "\n",
    "def reweigh_stopwords(word_weights, stoplist):\n",
    "    min_weight = get_min_weight\n",
    "    for word, weight in word_weights.items():\n",
    "        if word in stoplist:\n",
    "            word_weights[word] = weight/2\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Addition of english stop words\n",
    "\n",
    "def init_stopwords():\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    stop_words.add('&')\n",
    "    stop_words.add('co')\n",
    "    stop_words.add('co.')\n",
    "    stop_words.add('co.,')\n",
    "    stop_words.add('co.,ltd.')\n",
    "    stop_words.add('corp')\n",
    "    stop_words.add('corp.')\n",
    "    stop_words.add('corp.,')\n",
    "    stop_words.add('de')\n",
    "    stop_words.add('entertainment')\n",
    "    stop_words.add('family')\n",
    "    stop_words.add('foundation')\n",
    "    stop_words.add('inc')\n",
    "    stop_words.add('inc.')\n",
    "    stop_words.add('limited')\n",
    "    stop_words.add('industries')\n",
    "    stop_words.add('international')\n",
    "    stop_words.add('ltd')\n",
    "    stop_words.add('ltd.')\n",
    "    stop_words.add('s.a.')\n",
    "    stop_words.add('world')\n",
    "    stop_words.add('global')\n",
    "\n",
    "    stoplist = list(stop_words)\n",
    "    return stoplist\n",
    "\n",
    "stops = init_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_words(charity, shell, word_weights, average_weight, stoplist):\n",
    "    \n",
    "    \n",
    "    if charity is None or shell is None:\n",
    "        return False\n",
    "    \n",
    "    charity_words = [x.lower() for x in charity.split()]\n",
    "    shell_words = [x.lower() for x in shell.split()]\n",
    "    len_charity = len(charity_words)\n",
    "    len_shell = len(shell_words)\n",
    "    \n",
    "    threshold = average_weight\n",
    "    \n",
    "    nb_matches = 0\n",
    "    weight_matches = 0\n",
    "    nb_stopwords = 0\n",
    "    \n",
    "    for i in range(len_charity):\n",
    "        word = charity_words[i]\n",
    "        if word in shell_words and word in word_weights.keys():\n",
    "            weight_matches += word_weights.get(word)\n",
    "            nb_matches += 1\n",
    "            \n",
    "            if word in stoplist:\n",
    "                nb_stopwords += 1\n",
    "                \n",
    "    #note this also takes care of 0 matches\n",
    "    if nb_matches == nb_stopwords:\n",
    "            return False\n",
    "\n",
    "    if len_charity == 1 or len_shell == 1:\n",
    "        return (np.abs(len_charity - len_shell) < 2  and nb_matches == 1)\n",
    "            \n",
    "    if (weight_matches/nb_matches) >= threshold and (float(nb_matches)/float(len_shell)) > 0.6:\n",
    "        print('Charity: ', charity, ' Shell: ', shell)\n",
    "        print('Thresh: ', threshold, ' weight: ', weight_matches/nb_matches)\n",
    "            \n",
    "    return (weight_matches/nb_matches) >= threshold and (float(nb_matches)/float(len_shell)) > 0.6\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matches_between(leak, charity, sharp):\n",
    "    \n",
    "    stoplist = init_stopwords()\n",
    "    weights = get_word_weights()\n",
    "    reweigh_stopwords(word_weights, stoplist)\n",
    "    average_weight = get_average_weight(weights)\n",
    "    \n",
    "        \n",
    "    charity_location = '../generated/' + charity + '/' + charity + '_charity_info.csv'\n",
    "    leak_location = '../data/' + leak + '/' + leak + '*.nodes.entity.csv'\n",
    "    \n",
    "    leak_data = spark.read.csv(leak_location, header=True)\n",
    "\n",
    "    charity_data = spark.read.csv(charity_location, header=True)\n",
    "    \n",
    "    charity_names = charity_data.select('name', 'Headquarters').withColumnRenamed('name', 'CharityName')\n",
    "    shell_names = leak_data.select('node_id','name').withColumnRenamed('name', 'ShellName')\n",
    "    \n",
    "    shells_vs_charities = shell_names.crossJoin(charity_names)\n",
    "    \n",
    "    filtered_names = shells_vs_charities.rdd.filter(lambda r: check_for_words(r[1], r[2], weights,\n",
    "                                                                              average_weight, stoplist) == True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    matches = pd.DataFrame(filtered_names.collect(), columns=['node_id','ShellName','CharityName','CharityHeadquarters'])\n",
    "\n",
    "    \n",
    "    matches.to_csv('../generated/matches/' + leak +'_'+ charity +'_matches.csv')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "extract_matches_between('panama', 'forbes', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of possible charity names in the Panama Papers\n",
    "extract_matches_between('panama', 'forbes', False)\n",
    "extract_matches_between('panama', 'wikipedia', True)\n",
    "extract_matches_between('panama', 'INGO', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of possible charity names in the Paradise Papers\n",
    "extract_matches_between('paradise', 'forbes', False)\n",
    "extract_matches_between('paradise', 'wikipedia', True)\n",
    "extract_matches_between('paradise', 'INGO', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of possible charity names in the Bahamas Leaks\n",
    "extract_matches_between('bahamas', 'forbes', False)\n",
    "extract_matches_between('bahamas', 'wikipedia', True)\n",
    "extract_matches_between('bahamas', 'INGO', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of possible charity names in the Offshore Leaks\n",
    "extract_matches_between('offshore', 'forbes', False)\n",
    "extract_matches_between('offshore', 'wikipedia', True)\n",
    "extract_matches_between('offshore', 'INGO', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
