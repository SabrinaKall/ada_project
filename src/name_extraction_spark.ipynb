{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to find scraped charity names in the Panama Papers using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#stop words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "findspark.init('/opt/spark/spark-2.3.2-bin-hadoop2.7/')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "DATA_FOLDER = '../data'\n",
    "PANAMA_DATA_FOLDER = DATA_FOLDER + '/panama'\n",
    "\n",
    "GENERATED_FOLDER = '../generated'\n",
    "CHARITY_GENERATED_FOLDER = GENERATED_FOLDER + '/charities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and creation of dataframes\n",
    "pp_edges = spark.read.csv(PANAMA_DATA_FOLDER + '/panama_papers.edges.csv', header=True)\n",
    "pp_adress = spark.read.csv(PANAMA_DATA_FOLDER + '/panama_papers.nodes.address.csv', header=True)\n",
    "pp_entity = spark.read.csv(PANAMA_DATA_FOLDER + '/panama_papers.nodes.entity.csv', header=True)\n",
    "pp_intermediary = spark.read.csv(PANAMA_DATA_FOLDER + '/panama_papers.nodes.intermediary.csv', header=True)\n",
    "pp_officer = spark.read.csv(PANAMA_DATA_FOLDER + '/panama_papers.nodes.officer.csv', header=True)\n",
    "\n",
    "wiki_info = spark.read.csv(CHARITY_GENERATED_FOLDER + '/wikipedia_charity_info.csv', header=True)\n",
    "wiki_links =charities_info = spark.read.csv(CHARITY_GENERATED_FOLDER + '/wikipedia_charity_links.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Addition of english stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words.add('&')\n",
    "stop_words.add('co')\n",
    "stop_words.add('co.')\n",
    "stop_words.add('co.,')\n",
    "stop_words.add('co.,ltd.')\n",
    "stop_words.add('corp')\n",
    "stop_words.add('corp.')\n",
    "stop_words.add('corp.,')\n",
    "stop_words.add('de')\n",
    "stop_words.add('inc.')\n",
    "stop_words.add('foundation')\n",
    "stop_words.add('inc')\n",
    "stop_words.add('limited')\n",
    "stop_words.add('ltd')\n",
    "stop_words.add('ltd.')\n",
    "stop_words.add('s.a.')\n",
    "\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting each charity name and cleaning stop words\n",
    "\n",
    "def to_lower_parens_less(word):\n",
    "    return word.lower().replace('(', '').replace(')', '')\n",
    "\n",
    "charity_name = wiki_info.select('Name')\n",
    "def remove_stop(word_list):\n",
    "    return [to_lower_parens_less(w) for w in word_list if w.lower() not in stop_words]\n",
    "    \n",
    "#charity_name.replace(stop_words,None,inplace=True)\n",
    "charity_name_basic = charity_name.rdd.map(lambda r: remove_stop(r.Name))\n",
    "\n",
    "# Extracting company shell names\n",
    "\n",
    "shell_name=pp_entity.select('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "charities_vs_shells = charity_name.crossJoin(shell_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_words(charity, shell, percentage, lower_bound):\n",
    "    if charity is None or shell is None:\n",
    "        return False\n",
    "    \n",
    "    charity_words = charity.split()\n",
    "    shell_words = shell.split()\n",
    "    len_charity = len(charity_words)\n",
    "    len_shell = len(shell_words)\n",
    "    \n",
    "    count = 0\n",
    "    for word in charity_words:\n",
    "        if word in shell_words and word not in stop_words:\n",
    "            count += 1\n",
    "    return ((count/len_charity >= percentage) and (count/len_shell >= percentage))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, name: string]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charities_vs_shells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "condition should be string or Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-049f56edef4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcharities_vs_shells\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_for_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/spark-2.3.2-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: condition should be string or Column"
     ]
    }
   ],
   "source": [
    "filtered = charities_vs_shells.filter(lambda r: check_for_words(r.Name, r.name, 0.5, 3) == True)\n",
    "filtered.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This is the name cross_checking function in progress that will be loaded into the cluster. (Yes, we will improve on the for-loops. Probably.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'PipelinedRDD' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-263-a3e4bcc03e06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mFurthuer\u001b[0m \u001b[0mvisual\u001b[0m \u001b[0minspection\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mrequired\u001b[0m \u001b[0mto\u001b[0m \u001b[0meliminate\u001b[0m \u001b[0mfalse\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m '''\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshell_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharity_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'PipelinedRDD' has no len()"
     ]
    }
   ],
   "source": [
    "# Search of charity names in all shell names\n",
    "'''\n",
    "Matching names are found by accumuation of evidences.\n",
    "When two words of a charity name is found in a shell name,\n",
    "there is enough evidences that the name are at least very similar.\n",
    "Furthuer visual inspection may be required to eliminate false positives.\n",
    "'''\n",
    "for i in range(0,len(shell_name)):\n",
    "    for j in range(0, charity_name.shape[0]):\n",
    "        counter = 0\n",
    "        for k in range(0,charity_name.shape[1]):\n",
    "            if charity_name[k][j] is not None:\n",
    "                if re.search(' ' + charity_name[k][j] + ' ', shell_name[i]):\n",
    "                    counter = counter + 1\n",
    "                    if counter == 2:\n",
    "                        print('(' + str([i,j,k]) + ') ' + \"IT'S A MATCH ! WE'VE GOT THESE BASTARDS ! > \" + shell_name[i])\n",
    "                        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
