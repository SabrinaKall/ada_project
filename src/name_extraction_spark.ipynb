{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to find scraped charity names in the Panama Papers using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#stop words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "findspark.init(r'C:\\Users\\Ruijia\\Spark')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ruijia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Addition of english stop words\n",
    "\n",
    "def init_stopwords():\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    stop_words.add('&')\n",
    "    stop_words.add('co')\n",
    "    stop_words.add('co.')\n",
    "    stop_words.add('co.,')\n",
    "    stop_words.add('co.,ltd.')\n",
    "    stop_words.add('corp')\n",
    "    stop_words.add('corp.')\n",
    "    stop_words.add('corp.,')\n",
    "    stop_words.add('de')\n",
    "    stop_words.add('foundation')\n",
    "    stop_words.add('inc')\n",
    "    stop_words.add('inc.')\n",
    "    stop_words.add('limited')\n",
    "    stop_words.add('international')\n",
    "    stop_words.add('ltd')\n",
    "    stop_words.add('ltd.')\n",
    "    stop_words.add('s.a.')\n",
    "    stop_words.add('world')\n",
    "    stop_words.add('global')\n",
    "\n",
    "    stop_words = list(stop_words)\n",
    "    return stop_words\n",
    "\n",
    "stop_words = init_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_words(charity, shell, stop_words, tuning):\n",
    "    \n",
    "    percentage = 0.6\n",
    "    \n",
    "    if charity is None or shell is None:\n",
    "        return False\n",
    "    \n",
    "    charity_words = [x.lower() for x in charity.split()]\n",
    "    shell_words = [x.lower() for x in shell.split()]\n",
    "    len_charity = len(charity_words)\n",
    "    len_shell = len(shell_words)\n",
    "    \n",
    "    count_random_matches = 0\n",
    "    stop_word_random_matches = 0\n",
    "    \n",
    "    for i in range(len_charity):\n",
    "        word = charity_words[i]\n",
    "        if word in shell_words:\n",
    "            count_random_matches += 1\n",
    "            \n",
    "            if word in stop_words:\n",
    "                stop_word_random_matches += 1\n",
    "                \n",
    "    if tuning:\n",
    "        #if only stopwords match, not valid\n",
    "        if count_random_matches - stop_word_random_matches < 1:\n",
    "            return False\n",
    "\n",
    "        #\"Family foundations are tricky -> make sure they are not the only matching parts\"\n",
    "        if ('family' in shell_words \n",
    "            and 'foundation' in shell_words \n",
    "            and 'family' in charity_words \n",
    "            and 'foundation' in charity_words \n",
    "            and count_random_matches < 3 \n",
    "            and len_shell > 2 \n",
    "            and len_charity > 2):\n",
    "            return False\n",
    "\n",
    "    if len_charity == 1 or len_shell == 1:\n",
    "        return (np.abs(len_charity - len_shell) < 2  and count_random_matches == 1)\n",
    "        \n",
    "    return ((count_random_matches/len_charity >= percentage) \n",
    "            and (count_random_matches/len_shell >= percentage))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matches_between(leak, charity, sharp):\n",
    "    \n",
    "    stop_words = init_stopwords()\n",
    "    \n",
    "    charity_location = '../generated/' + charity + '/' + charity + '_charity_info.csv'\n",
    "    leak_location = '../data/' + leak + '/' + leak + '*.nodes.entity.csv'\n",
    "    \n",
    "    leak_data = spark.read.csv(leak_location, header=True)\n",
    "\n",
    "    charity_data = spark.read.csv(charity_location, header=True)\n",
    "    \n",
    "    charity_names = charity_data.select('name', 'Headquarters').withColumnRenamed('name', 'CharityName')\n",
    "    shell_names = leak_data.select('node_id','name').withColumnRenamed('name', 'ShellName')\n",
    "    \n",
    "    shells_vs_charities = shell_names.crossJoin(charity_names)\n",
    "    \n",
    "    filtered_names = shells_vs_charities.rdd.filter(lambda r: check_for_words(r[1], r[2], stop_words, sharp) == True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    matches = pd.DataFrame(filtered_names.collect(), columns=['node_id','ShellName','CharityName','CharityHeadquarters'])\n",
    "\n",
    "    \n",
    "    matches.to_csv('../generated/matches/' + leak +'_'+ charity +'_matches.csv')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ruijia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Panama Papers\n",
    "extract_matches_between('panama', 'forbes', False)\n",
    "extract_matches_between('panama', 'wikipedia', True)\n",
    "extract_matches_between('panama', 'INGO', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of possible charity names in the Paradise Papers\n",
    "extract_matches_between('paradise', 'forbes', False)\n",
    "extract_matches_between('paradise', 'wikipedia', True)\n",
    "extract_matches_between('paradise', 'INGO', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of possible charity names in the Bahamas Leaks\n",
    "extract_matches_between('bahamas', 'forbes', False)\n",
    "extract_matches_between('bahamas', 'wikipedia', True)\n",
    "extract_matches_between('bahamas', 'INGO', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of possible charity names in the Offshore Leaks\n",
    "extract_matches_between('offshore', 'forbes', False)\n",
    "extract_matches_between('offshore', 'wikipedia', True)\n",
    "extract_matches_between('offshore', 'INGO', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
