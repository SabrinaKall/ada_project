{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to find scraped charity names in Leaked Papers using Spark\n",
    "\n",
    "Using the names of charities from wikipedia and forbes (see web_scraping section), we filter the leaked nodes for shell companies with names of a certain degree of similarity to those of the charities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#stop words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "\n",
    "#Ruijia\n",
    "#findspark.init(r'C:\\Users\\Ruijia\\Spark')\n",
    "\n",
    "#Sabrina\n",
    "findspark.init('/opt/spark/spark-2.3.2-bin-hadoop2.7/')\n",
    "\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Part of our filtering algorithm checks nltk's list of stopwords, augmented with handpicked results from trial-and-error, to discount matches between shell companies and charities that may have occurred without good reason. (For example matching only on words like \"of\" and \"the\" is clearly not valid.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "\n",
    "def init_stopwords():\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    stop_words.add('&')\n",
    "    stop_words.add('co')\n",
    "    stop_words.add('co.')\n",
    "    stop_words.add('co.,')\n",
    "    stop_words.add('co.,ltd.')\n",
    "    stop_words.add('corp')\n",
    "    stop_words.add('corp.')\n",
    "    stop_words.add('corp.,')\n",
    "    stop_words.add('de')\n",
    "    stop_words.add('foundation')\n",
    "    stop_words.add('inc')\n",
    "    stop_words.add('inc.')\n",
    "    stop_words.add('limited')\n",
    "    stop_words.add('international')\n",
    "    stop_words.add('ltd')\n",
    "    stop_words.add('ltd.')\n",
    "    stop_words.add('s.a.')\n",
    "    stop_words.add('world')\n",
    "    stop_words.add('global')\n",
    "\n",
    "    stop_words = list(stop_words)\n",
    "    return stop_words\n",
    "\n",
    "stop_words = init_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Matching Algorithm\n",
    "\n",
    "Here is our actual matching algorithm, improved mostly by trial and error (we attempted to use idf scores, but this had a terrible performance).\n",
    "\n",
    "\n",
    "Given a shell name and charity name, we do as follows:\n",
    "\n",
    " 1. Count the number of words in common between the two names, and the number of words in common that are stopwords\n",
    " 2. If tuning is set to occur (this is optimized for wikipedia charities)\n",
    "     1. If all the matches are stopwords -> not a match\n",
    "     2. If \"family foundation\" are the only matching words (these occur very, very often, so we make a special case) for them -> not a match\n",
    "     3. If one of the names is only one word:\n",
    "         1. Does the word occur in the other name (and is it not a stopword)? -> match\n",
    "         2. Else not a match\n",
    "     4. If the percentage of matches over the total length of the names is above a certain threshold (we found 0.6 to be optimal) -> match\n",
    "     5. Else not a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_for_words(charity, shell, stop_words, tuning):\n",
    "    \n",
    "    percentage = 0.6\n",
    "    \n",
    "    if charity is None or shell is None:\n",
    "        return False\n",
    "    \n",
    "    charity_words = [x.lower() for x in charity.split()]\n",
    "    shell_words = [x.lower() for x in shell.split()]\n",
    "    len_charity = len(charity_words)\n",
    "    len_shell = len(shell_words)\n",
    "    \n",
    "    count_random_matches = 0\n",
    "    stop_word_random_matches = 0\n",
    "    \n",
    "    if len_charity == 0 or len_shell == 0:\n",
    "        return False\n",
    "    \n",
    "    for i in range(len_charity):\n",
    "        word = charity_words[i]\n",
    "        if word in shell_words:\n",
    "            count_random_matches += 1\n",
    "            \n",
    "            if word in stop_words:\n",
    "                stop_word_random_matches += 1\n",
    "                \n",
    "    if tuning:\n",
    "        #if only stopwords match, not valid\n",
    "        if count_random_matches - stop_word_random_matches < 1:\n",
    "            return False\n",
    "\n",
    "        #\"Family foundations are tricky -> make sure they are not the only matching parts\"\n",
    "        if ('family' in shell_words \n",
    "            and 'foundation' in shell_words \n",
    "            and 'family' in charity_words \n",
    "            and 'foundation' in charity_words \n",
    "            and count_random_matches < 3 \n",
    "            and len_shell > 2 \n",
    "            and len_charity > 2):\n",
    "            return False\n",
    "\n",
    "    if len_charity == 1 or len_shell == 1:\n",
    "        return (np.abs(len_charity - len_shell) < 2  and count_random_matches == 1)\n",
    "        \n",
    "    return ((count_random_matches/len_charity >= percentage) \n",
    "            and (count_random_matches/len_shell >= percentage))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Algorithm\n",
    "\n",
    "With this function, we can search a specific set of leaked papers (panama, paradise, bahamas or offshore), a specific source of charities (wikipedia, forbes or INGO) and a specific type of node (officer or entity) for matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matches_between(leak, charity, sharp, node_type):\n",
    "    \n",
    "    \n",
    "    stop_words = init_stopwords()\n",
    "    \n",
    "    #load data\n",
    "    charity_location = '../generated/scraping/' + charity + '/' + charity + '_charity_info.csv'\n",
    "    leak_location = '../data/' + leak + '/' + leak + '*.nodes.'+ node_type +'.csv'\n",
    "    \n",
    "    leak_data = spark.read.csv(leak_location, header=True)\n",
    "\n",
    "    charity_data = spark.read.csv(charity_location, header=True)\n",
    "    \n",
    "    #select columns\n",
    "    charity_names = charity_data.select('name', 'Headquarters').withColumnRenamed('name', 'CharityName')\n",
    "    shell_names = leak_data.select('node_id','name').withColumnRenamed('name', 'ShellName')\n",
    "    \n",
    "    #crossjoin lists of names\n",
    "    shells_vs_charities = shell_names.crossJoin(charity_names)\n",
    "    \n",
    "    #filter the crossjoined names using the algorithm\n",
    "    filtered_names = shells_vs_charities.rdd.filter(lambda r: check_for_words(r[1], r[2], stop_words, sharp) == True)\n",
    "    \n",
    "    #collect and write to file the results\n",
    "    matches = pd.DataFrame(filtered_names.collect(), columns=['node_id','ShellName','CharityName','CharityHeadquarters'])\n",
    "\n",
    "    \n",
    "    matches.to_csv('../generated/matches/' + node_type +'/'+ node_type +'_'+ leak +'_'+ charity +'_matches.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity nodes\n",
    "\n",
    "We search all combinations of leaks and charity files according to nodes of type entity, as these are the ones that usually represent shell companies and most of the charities are to be found here, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Panama Papers\n",
    "extract_matches_between('panama', 'forbes', False, 'entity')\n",
    "extract_matches_between('panama', 'wikipedia', True, 'entity')\n",
    "extract_matches_between('panama', 'INGO', False, 'entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Paradise Papers\n",
    "extract_matches_between('paradise', 'forbes', False,'entity')\n",
    "extract_matches_between('paradise', 'wikipedia', True, 'entity')\n",
    "extract_matches_between('paradise', 'INGO', False, 'entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Bahamas Leaks\n",
    "extract_matches_between('bahamas', 'forbes', False, 'entity')\n",
    "extract_matches_between('bahamas', 'wikipedia', True, 'entity')\n",
    "extract_matches_between('bahamas', 'INGO', False, 'entity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Offshore Leaks\n",
    "extract_matches_between('offshore', 'forbes', False, 'entity')\n",
    "extract_matches_between('offshore', 'wikipedia', True, 'entity')\n",
    "extract_matches_between('offshore', 'INGO', False, 'entity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Officer nodes\n",
    "\n",
    "We also search \"officer\" type nodes, as data exploration showed that these can also contain the names of charities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Panama Papers\n",
    "extract_matches_between('panama', 'forbes', False, 'officer')\n",
    "extract_matches_between('panama', 'wikipedia', True, 'officer')\n",
    "extract_matches_between('panama', 'INGO', False, 'officer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Paradise Papers\n",
    "extract_matches_between('paradise', 'forbes', False, 'officer')\n",
    "extract_matches_between('paradise', 'wikipedia', True, 'officer')\n",
    "extract_matches_between('paradise', 'INGO', False, 'officer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Bahamas Leaks\n",
    "extract_matches_between('bahamas', 'forbes', False, 'officer')\n",
    "extract_matches_between('bahamas', 'wikipedia', True, 'officer')\n",
    "extract_matches_between('bahamas', 'INGO', False, 'officer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Extraction of possible charity names in the Offshore Leaks\n",
    "extract_matches_between('offshore', 'forbes', False, 'officer')\n",
    "extract_matches_between('offshore', 'wikipedia', True, 'officer')\n",
    "extract_matches_between('offshore', 'INGO', False, 'officer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
