{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency\n",
    "\n",
    "Create a dictionary of words appearing in charity names mapped to their inverse logarithmic frequency.\n",
    "\n",
    "-> The more often a word appears in a charity names, the less weight we attribute to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#stop words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#spark\n",
    "import findspark\n",
    "#Ruijia\n",
    "#findspark.init(r'C:\\Users\\Ruijia\\Spark')\n",
    "\n",
    "#Sabrina\n",
    "findspark.init('/opt/spark/spark-2.3.2-bin-hadoop2.7/')\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "def get_charity_info_location(charity):\n",
    "    return '../generated/' + charity + '/' + charity + '_charity_info.csv'\n",
    "    \n",
    "forbes_names = pd.read_csv(get_charity_info_location('forbes'))['name'].tolist()\n",
    "wikipedia_names = pd.read_csv(get_charity_info_location('wikipedia'))['name'].tolist()\n",
    "INGO_names = pd.read_csv(get_charity_info_location('INGO'))['name'].tolist()\n",
    "\n",
    "names = forbes_names + wikipedia_names + INGO_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forbes length:  100\n",
      "Wikipedia length:  324\n",
      "INGO length:  45\n",
      "Total number of names:  469\n"
     ]
    }
   ],
   "source": [
    "print('Forbes length: ', len(forbes_names))\n",
    "print('Wikipedia length: ', len(wikipedia_names))\n",
    "print('INGO length: ', len(INGO_names))\n",
    "\n",
    "print('Total number of names: ', len(names))\n",
    "\n",
    "nb_names = len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of individual words\n",
    "words = \" \".join(names).split()\n",
    "nb_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {} \n",
    "for word in words: \n",
    "    word = word.lower()\n",
    "    if (word in word_freq): \n",
    "        word_freq[word] += 1\n",
    "    else: \n",
    "        word_freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF(t) = log_e(Total number of documents (here names) / Number of documents/names with term t in it). \n",
    "def idf(word_count):\n",
    "    return np.log(nb_names/word_count)\n",
    "    \n",
    "\n",
    "word_weights = {k: idf(v) for k, v in word_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of word \"foundation\":  1.252762968495368\n",
      "Weight of word \"amnesty\":  6.150602768446279\n",
      "Weight of word \"the\":  2.389402652752717\n"
     ]
    }
   ],
   "source": [
    "print('Weight of word \"foundation\": ', word_weights.get('foundation'))\n",
    "print('Weight of word \"amnesty\": ', word_weights.get('amnesty'))\n",
    "print('Weight of word \"the\": ', word_weights.get('of'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, common words like \"foundation\" and \"the\" have waker weights than rare words like \"amnesty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.150602768446279"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_weight = 0\n",
    "for weight in word_weights.values():\n",
    "    if weight > max_weight:\n",
    "        max_weight = weight\n",
    "\n",
    "max_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
